{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All this notebook has been used on  google collab platform and NOT localy using T4 GPU\n",
    "\n",
    "!pip install fastapi \n",
    "!pip install colabcode\n",
    "!pip install uvicorn==0.15.0\n",
    "!pip install pyngrok==5.0.0\n",
    "\n",
    "!ngrok authtoken 2VxpDsoeuVw9C5BCDjoZeiH89Iq_2TGwbQGBKYYu7oo26PAwv\n",
    "!ngrok http -host-header=rewrite localhost:PORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.*\")\n",
    "import os\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, AutoModelForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau , CosineAnnealingLR\n",
    "from sklearn.metrics import f1_score, jaccard_score\n",
    "import ast\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy\n",
    "\n",
    "import random\n",
    "\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "def preprocessed_text(text):\n",
    "    # Define the regular expression pattern for French punctuation marks\n",
    "    punctuation_pattern = r'[,?!;:…()\\[\\]«»—/\"\"\\'\\']'\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text_without_tags = soup.get_text()\n",
    "    \n",
    "    # Use re.sub() to remove the matched punctuation marks\n",
    "    cleaned_text = re.sub(punctuation_pattern, '', text_without_tags)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(cleaned_text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens \n",
    "              if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(model, input_question, threshold=0.2):\n",
    "    input_question.lower()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    input_ids = tokenizer.encode(input_question, max_length=tokenizer.model_max_length, truncation=True)\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    \n",
    "    predicted_probabilities = torch.nn.functional.sigmoid(outputs.logits)   # Sigmoid because  it is a  multilabel text classification,and softmax if it is a  mutli class task\n",
    "    tag_probabilities = predicted_probabilities.squeeze().tolist()\n",
    "    \n",
    "    total_probability = round(sum(tag_probabilities) , 6)\n",
    "\n",
    "    tag_labels = y.columns  \n",
    "\n",
    "    decimal_places = 3\n",
    "    \n",
    "    tag_predictions = [{\"tag\": tag_labels[i], \"probability\": prob} for i, prob in enumerate(tag_probabilities) if prob > threshold]\n",
    "    \n",
    "    sorted_tag_predictions = sorted(tag_predictions, key=lambda x: x[\"probability\"], reverse=True)\n",
    "\n",
    "    print(f\"Here is the sum of the total probability of all 60 tags :  {total_probability} \" )\n",
    "\n",
    "    # Print the sorted tag predictions above the threshold\n",
    "    for tag_prediction in sorted_tag_predictions:\n",
    "        rounded_probability = round(tag_prediction['probability'], decimal_places)\n",
    "        print(f\"Tag: {tag_prediction['tag']}, Probability: {rounded_probability}\")\n",
    "    \n",
    "    #returning the suggested tags in an array format\n",
    "    return [item['tag'] for item in sorted_tag_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=labels_number\n",
    ")\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "model_path = \"/content/drive/My Drive/bert_model_2e-5.pth\"\n",
    "\n",
    "loaded_model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException , Query\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "from pyngrok import ngrok\n",
    "import nest_asyncio\n",
    "from colabcode import ColabCode\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_model.to(device)\n",
    "\n",
    "loaded_model.eval()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "class QuestionInput(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.get(\"/predict/\")\n",
    "async def predicting_tags(question: str = Query(..., description=\"Input question for prediction\")):\n",
    "    # Preprocess the input question\n",
    "    preprocessed_question = preprocessed_text(question)\n",
    "\n",
    "    # Predict tags\n",
    "    tags = predict_tags(loaded_model, preprocessed_question , threshold=0.1)\n",
    "    \n",
    "    # Create a response string that combines the question and tags\n",
    "    question_text = f\"Input Question: {question}\"\n",
    "    tags_text = f\"Predicted Tags: {', '.join(tags)}\"\n",
    "\n",
    "   # Combine response_text and tags_text with a newline character\n",
    "    combined_response = question_text + \" |¤| \" + tags_text\n",
    "\n",
    "    # Return the response text as plain text\n",
    "    return combined_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the FastAPI app using ColabCode\n",
    "cc = ColabCode(port=8000, code=False)\n",
    "cc.run_app(app=app)\n",
    "\n",
    "\n",
    "# Add \"/predict/?question=How can I input an image into my website ?\" to the ngrok tunnel URL to input a question for example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
