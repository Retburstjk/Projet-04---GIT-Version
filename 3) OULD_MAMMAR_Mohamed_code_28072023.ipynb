{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import re\n",
    "import pyLDAvis.gensim\n",
    "import  numpy  as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import string\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOWER CASE DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def lower_case_df(text_to_lower):\n",
    "    if isinstance(text_to_lower, str):\n",
    "        tokens = word_tokenize(text_to_lower)\n",
    "        lowercased_tokens = [token.lower() for token in tokens]\n",
    "        return ' '.join(lowercased_tokens)\n",
    "    else:\n",
    "        return text_to_lower\n",
    "\n",
    "# Example usage:\n",
    "random_text = 'ThiS iS A tExT tO TEsT ThE fUnCTioN oF LOWerINg cAsE oF aLL LeTTErS'\n",
    "lowercased_string = lower_case_df(random_text)\n",
    "\n",
    "print(random_text)\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "print(lowercased_string)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVE HTLM SIGNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_htmlTags_punctuation(text):\n",
    "  \n",
    "    # Define the regular expression pattern for French punctuation marks\n",
    "    punctuation_pattern = r'[,?!;:…()\\[\\]«»—/\"\"\\'\\']'\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text_without_tags = soup.get_text()\n",
    "    \n",
    "    # Use re.sub() to remove the matched punctuation marks\n",
    "    cleaned_text = re.sub(punctuation_pattern, '', text_without_tags)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage:\n",
    "html_text = '<p>Voici un exemple de texte avec des ponctuations et des balises HTLM : «Salut !» et \\'Bonjour !\\'</p>'\n",
    "cleaned_text = remove_htmlTags_punctuation(html_text)\n",
    "print(html_text)\n",
    "print(\"--------------------------------------------------------------------------------------------------------\")\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEMMATIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(text):\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens \n",
    "              if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Sample text\n",
    "text = \"This is an example sentence that we'll use for text cleaning. It includes stopwords and different verb tenses.\"\n",
    "\n",
    "\n",
    "cleaned_text = lemmatizer(text)\n",
    "\n",
    "print(text)\n",
    "print(\"------------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL PREPROCESS FUNCTIONS INTO ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def preprocessed_text(text):\n",
    "    # Define the regular expression pattern for French punctuation marks\n",
    "    punctuation_pattern = r'[,?!;:…()\\[\\]«»—/\"\"\\'\\']'\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text_without_tags = soup.get_text()\n",
    "    \n",
    "    # Use re.sub() to remove the matched punctuation marks\n",
    "    cleaned_text = re.sub(punctuation_pattern, '', text_without_tags)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(cleaned_text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens \n",
    "              if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Sample text with HTML tags and punctuation\n",
    "sample_text = \"\"\"\n",
    "<html>\n",
    "<head><title>Sample Text</title></head>\n",
    "<body>\n",
    "<p>This is an example of text with HTML tags, punctuation, and some stop words. It's a sample text for preprocessing.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Call the preprocessed_text function to preprocess the sample text\n",
    "preprocessed_sample = preprocessed_text(sample_text)\n",
    "\n",
    "print(sample_text)\n",
    "print(\"------------------------------------------------------------------------------------------------------------------------\")\n",
    "# Display the preprocessed text\n",
    "print(preprocessed_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a MLB Dataframe with the top X most frequents tags with atleast a 1 in the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_by_tags(df, all_tags, X):\n",
    "    # Step 4: Take the first X tags from the list\n",
    "    first_tags = all_tags[:X]\n",
    "\n",
    "    # Step 5: Filter the DataFrame based on the first X tags\n",
    "    filtered_df = df[df[first_tags].any(axis=1)]\n",
    "\n",
    "    # Step 6: Drop all columns beyond the first X tags\n",
    "    filtered_df = filtered_df.drop(columns=all_tags[X:])\n",
    "    filtered_df = pd.DataFrame(filtered_df)\n",
    "    \n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reading / Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DF_OG = pd.read_csv(\"DF_OG.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_OG.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_OG.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_OG.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tags from each row using regular expression\n",
    "tags_list = [re.findall(r'<(.*?)>', row) for row in DF_OG['Tags']]\n",
    "\n",
    "combined_tags = [tag for sublist in tags_list for tag in sublist]\n",
    "\n",
    "# Convert the list into a set to retain unique tags\n",
    "unique_tags = set(combined_tags)\n",
    "\n",
    "unique_tags_array = np.array(unique_tags)\n",
    "\n",
    "unique_tags_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csharp_rows = DF_OG[DF_OG['Body'].str.contains('c#', case=False)]\n",
    "csharp_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lemmatizer = pd.DataFrame([])\n",
    "test_lemmatizer['Title'] = DF_OG['Title'].apply(remove_htmlTags_punctuation)\n",
    "test_lemmatizer['Body'] = DF_OG['Body'].apply(remove_htmlTags_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csharp_rows = test_lemmatizer[test_lemmatizer['Body'].str.contains('c#', case=False)]\n",
    "csharp_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_lemmatizer['Title'] = test_lemmatizer['Title'].apply(lemmatizer)\n",
    "test_lemmatizer['Body'] = test_lemmatizer['Body'].apply(lemmatizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csharp_rows = test_lemmatizer[test_lemmatizer['Body'].str.contains('c #', case=False)]\n",
    "csharp_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(np.log(DF_OG['ViewCount']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_OG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the removing HTML signs to the 'Questions' column\n",
    "DF_OG['Title'] = DF_OG['Title'].apply(preprocessed_text)\n",
    "DF_OG['Body'] = DF_OG['Body'].apply(preprocessed_text)\n",
    "\n",
    "\n",
    "df_MultiLabel = DF_OG[['Title' , 'Body' , 'Tags']]\n",
    "\n",
    "df_MultiLabel['Questions'] = df_MultiLabel['Title'] + \" \" + df_MultiLabel['Body']\n",
    "# df_MultiLabel['Questions'] = df_MultiLabel['Questions'].apply(remove_html_tags)\n",
    "\n",
    "\n",
    "df_MultiLabel = df_MultiLabel.drop(['Title' , 'Body'] , axis=1)\n",
    "# Swap the positions of columns Tags and Questions\n",
    "df_MultiLabel['Tags'], df_MultiLabel['Questions'] = df_MultiLabel['Questions'], df_MultiLabel['Tags'].copy()\n",
    "# Swap the positions and column names of columns B and C\n",
    "df_MultiLabel = df_MultiLabel.rename(columns={'Tags': 'Questions', \n",
    "                                  'Questions': 'Tags'})\n",
    "\n",
    "df_MultiLabel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"c #\" with \"c#\"\n",
    "df_MultiLabel['Questions'] = df_MultiLabel['Questions'].str.replace('c #', 'c#')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tags from each row using regular expression\n",
    "tags_list = [re.findall(r'<(.*?)>', row) for row in df_MultiLabel['Tags']]\n",
    "\n",
    "# Combine all tags into a single list\n",
    "combined_tags = [tag for sublist in tags_list for tag in sublist]\n",
    "\n",
    "# Convert the list into a set to retain unique tags\n",
    "unique_tags = set(combined_tags)\n",
    "\n",
    "# Count the number of unique tags\n",
    "num_unique_tags = len(unique_tags)\n",
    "\n",
    "print(\"Number of unique tags:\", num_unique_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each tag\n",
    "tag_counts = {}\n",
    "for tag in combined_tags:\n",
    "    if tag in tag_counts:\n",
    "        tag_counts[tag] += 1\n",
    "    else:\n",
    "        tag_counts[tag] = 1\n",
    "\n",
    "# Sort the tags based on their counts in descending order\n",
    "sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Repeated tags (sorted by count) FORMAT => TAG : COUNT:\")\n",
    "for tag, count in sorted_tags:\n",
    "    print(f\"Tag: {tag} : {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each tag\n",
    "tag_counts = pd.Series(combined_tags).value_counts()\n",
    "\n",
    "# Sort the counts in descending order\n",
    "sorted_counts = tag_counts.sort_values(ascending=False)\n",
    "\n",
    "# Calculate the sum of the repeated numbers\n",
    "sum_repeated_numbers = sorted_counts.sum()\n",
    "\n",
    "print(\"Sum of repeated numbers:\", sum_repeated_numbers)\n",
    "\n",
    "# Generate normal numbers for x-axis\n",
    "x_values = list(range(1, len(sorted_counts) + 1))\n",
    "\n",
    "# Plot the graph using sns.histplot\n",
    "sns.histplot(x=x_values, bins='auto', weights=sorted_counts.values, kde=False, color='b')\n",
    "plt.xlabel('Number')\n",
    "plt.ylabel('Repetition Count')\n",
    "plt.title('Repetition Count Histogram')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_values = pd.DataFrame(sorted_tags)[1]\n",
    "\n",
    "# Slice the list to take the first 100 values\n",
    "first_100_values = repeated_values[:100]\n",
    "\n",
    "# Create a plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(first_100_values) + 1), first_100_values)\n",
    "\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Graph of First 100 Repeated Values')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MultiLabel['Tags'] = df_MultiLabel['Tags'].str.findall(r'<(.*?)>')\n",
    "\n",
    "# df_MultiLabel['Questions'] = df_MultiLabel['Questions'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "all_tags = pd.DataFrame(sorted_tags)[0]\n",
    "\n",
    "df_non_MLB =  df_MultiLabel\n",
    "df_non_MLB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Labeling the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLB = MultiLabelBinarizer(classes=all_tags)\n",
    "tags_binary = MLB.fit_transform(df_MultiLabel['Tags'])\n",
    "\n",
    "# Convert the binary representation to a DataFrame\n",
    "tags_df = pd.DataFrame(tags_binary, columns=MLB.classes_)\n",
    "\n",
    "# Step 3: Concatenate the binary representation to the original DataFrame\n",
    "df_MultiLabel = pd.concat([df_MultiLabel, tags_df], axis=1)\n",
    "df_MultiLabel.drop('Tags' , axis=1 , inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the value of how many tags to have 90% of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_targeted_value = 90\n",
    "\n",
    "filtered_rows_percent = []  # List to store filtered_df.shape[0] values in percentage\n",
    "\n",
    "for labels_number in range(101):\n",
    "    filtered_df = filter_by_tags(df_MultiLabel, all_tags, labels_number)\n",
    "    percent = (filtered_df.shape[0] / DF_OG.shape[0]) * 100  # Convert to percentage\n",
    "    filtered_rows_percent.append(percent)\n",
    "\n",
    "# Find the index where the Y value is closest to 90%\n",
    "index_90_percent = filtered_rows_percent.index(min(filtered_rows_percent, key=lambda x: abs(x - percentage_targeted_value)))\n",
    "x_90_percent = index_90_percent  # X value where Y is closest to 90%\n",
    "y_90_percent = filtered_rows_percent[index_90_percent]  # Y value closest to 90%\n",
    "\n",
    "# Create a plot with  90% threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(101), filtered_rows_percent, marker='o')\n",
    "\n",
    "plt.axhline(y=percentage_targeted_value, color='g', linestyle='--', label='90% Threshold')\n",
    "plt.axvline(x=x_90_percent, color='g', linestyle='--', label=f'X at {x_90_percent}')\n",
    "plt.annotate(f'({x_90_percent}, {y_90_percent:.2f}%)', (x_90_percent, y_90_percent + 2), color='g', xytext=(x_90_percent + 1, y_90_percent + 4))\n",
    "\n",
    "plt.xlabel('Number of Tags')\n",
    "plt.ylabel('Taille du data frame (%)')\n",
    "plt.title('Nombre de Tags en feature par rapport a la taille original (%)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_number = 60\n",
    "#Takign top 60 tags where there is atleast 1 on those tags\n",
    "filtered_df = filter_by_tags(df_MultiLabel  , all_tags , labels_number)\n",
    "\n",
    "# Sample X % of the data (you can adjust the fraction as needed)\n",
    "sample_df = filtered_df.sample(frac=1, random_state=42)\n",
    "df_MLB = sample_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop the \"Questions\" column\n",
    "sample_df_without_questions = df_MLB.drop(columns=[\"Questions\"])\n",
    "\n",
    "# Sum of feature values excluding the \"Questions\" column\n",
    "sum_of_features = sample_df_without_questions.sum()\n",
    "\n",
    "# Check if any sum is equal to 0\n",
    "any_zero_value = (sum_of_features == 0).any()\n",
    "\n",
    "print(\"Sum of feature values:\")\n",
    "print(sum_of_features)\n",
    "print(\"\\nAre there any sums equal to 0?\")\n",
    "print(any_zero_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_display = sample_df.index.tolist()  # Get the list of index values from sample_df\n",
    "\n",
    "# Create a new DataFrame containing the rows with the specified index values from df_non_MLB\n",
    "df_non_MLB = df_non_MLB.loc[indices_to_display]\n",
    "df_non_MLB = df_non_MLB.sort_index()\n",
    "\n",
    "df_OG_TT = DF_OG[['Title' , 'Body' , 'Tags']]\n",
    "df_OG_TT = df_OG_TT.loc[indices_to_display]\n",
    "df_OG_TT = df_OG_TT.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OG_TT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_MLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csharp_rows = df_non_MLB[df_non_MLB['Questions'].str.contains('c#', case=False)]\n",
    "csharp_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OG_TT['Tags'] = df_OG_TT['Tags'].str.findall(r'<(.*?)>')\n",
    "df_OG_TT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MLB.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import gensim\n",
    "\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import corpora, models\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import ast\n",
    "import  numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "import string\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process the text data\n",
    "def process_text(texts):\n",
    "    \n",
    "    bigram = Phrases(texts, min_count=5, threshold=100)\n",
    "    trigram = Phrases(bigram[texts], min_count=5, threshold=100)\n",
    "\n",
    "    # Apply the bigram and trigram models\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    trigram_mod = Phraser(trigram)\n",
    "    \n",
    "    # Apply bigram and trigram models\n",
    "    texts_P = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = gensim.parsing.preprocessing.STOPWORDS\n",
    "    texts_F = [[word for word in doc if word not in stop_words] for doc in texts_P]\n",
    "    \n",
    "    return texts_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed_text(text):\n",
    "    # Define the regular expression pattern for French punctuation marks\n",
    "    punctuation_pattern = r'[,?!;:…()\\[\\]«»—/\"\"\\'\\']'\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text_without_tags = soup.get_text()\n",
    "    \n",
    "    # Use re.sub() to remove the matched punctuation marks\n",
    "    cleaned_text = re.sub(punctuation_pattern, '', text_without_tags)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(cleaned_text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens \n",
    "              if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre building of the LDA model with bi/trigram  phraser and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LDA = df_non_MLB[['Questions']]\n",
    "\n",
    "# df_LDA['Questions'] =  df_LDA['Questions'].apply(clean_text)\n",
    "# Preprocess the text data\n",
    "df_LDA['text'] = df_LDA['Questions']\n",
    "df_LDA['clean_text'] = df_LDA['text'].apply(gensim.utils.simple_preprocess, deacc=True)\n",
    "\n",
    "# Create bigrams and trigrams\n",
    "text_data = df_LDA['clean_text'].tolist()\n",
    "\n",
    "bigram = Phrases(text_data, min_count=5, threshold=100)\n",
    "trigram = Phrases(bigram[text_data], min_count=5, threshold=100)\n",
    "\n",
    "# Apply the bigram and trigram models\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)\n",
    "\n",
    "# Apply bigram and trigram models\n",
    "texts_P = [trigram_mod[bigram_mod[doc]] for doc in text_data]\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = gensim.parsing.preprocessing.STOPWORDS\n",
    "texts_F = [[word for word in doc if word not in stop_words] for doc in texts_P]\n",
    "\n",
    "# Create the dictionary\n",
    "id2word = corpora.Dictionary(texts_F)\n",
    "\n",
    "# Create the corpus with TF-IDF representation\n",
    "texts = [id2word.doc2bow(doc) for doc in texts_F]\n",
    "\n",
    "tfidf = models.TfidfModel(texts)\n",
    "corpus_tfidf = tfidf[texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_word = \"c#\"\n",
    "# count = sum(1 for word in processed_data if search_word in word)\n",
    "\n",
    "# print(\"Number of occurrences of '\",search_word ,\" ':\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating  the  coherence score iterating the num-topics parameter  of the  LDA model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_scores = []\n",
    "num_topics_range = range(1, 100, 1)  # Range of num_topics values from 2 to 60 jumping by 2\n",
    "\n",
    "for num_topics in num_topics_range:\n",
    "    # Train the LDA model (using LdaMulticore)\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus_tfidf,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics,\n",
    "                                           random_state=100,\n",
    "                                           passes=10,\n",
    "                                           workers=4)\n",
    "\n",
    "    # Compute coherence score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts_F, dictionary=id2word, coherence='c_v')\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    coherence_scores.append(coherence_score)\n",
    "\n",
    "    print(\"num_topics:\", num_topics, \"Coherence Score:\", coherence_score)\n",
    "\n",
    "\n",
    "# Plot the coherence scores against alpha values\n",
    "plt.figure(figsize=(15, 9))\n",
    "# Plot the coherence scores\n",
    "plt.plot(num_topics_range, coherence_scores, marker='o')\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.title(\"Coherence Score vs num_topics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating  the  coherence score iterating the alpha parameter  of the  LDA model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of alpha values to test\n",
    "alpha_values = np.arange(0.1, 0.51, 0.01)\n",
    "\n",
    "# List to store coherence scores for each alpha\n",
    "coherence_scores = []\n",
    "\n",
    "# Train LDA models and calculate coherence scores\n",
    "for alpha in alpha_values:\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus_tfidf,\n",
    "                                           id2word=id2word,\n",
    "                                           alpha=alpha,\n",
    "                                           num_topics=41,  # Adjust the number of topics as desired\n",
    "                                           random_state=100,\n",
    "                                           passes=10,\n",
    "                                           workers=4)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts_F, dictionary=id2word, coherence='c_v')\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    coherence_scores.append(coherence_score)\n",
    "    \n",
    "    print(f\"Alpha: {alpha:.2f} - Coherence Score: {coherence_score:.4f}\")\n",
    "\n",
    "# Plot the coherence scores against alpha values\n",
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(alpha_values, coherence_scores, marker='o')\n",
    "plt.xlabel('Alpha Value')\n",
    "plt.ylabel('Coherence Score')\n",
    "plt.title('LDA Model Coherence Score vs. Alpha Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the LDA model with optimized parameters (alpha=0.43   , num_topics=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LDA model (using LdaMulticore)\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus_tfidf,\n",
    "                                       id2word=id2word,\n",
    "                                       alpha=0.43,\n",
    "                                       num_topics=41,  # Adjust the number of topics as desired\n",
    "                                       random_state=100,\n",
    "                                       passes=10,\n",
    "                                       workers=4)\n",
    "\n",
    "# Compute coherence score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts_F, dictionary=id2word, coherence='c_v')\n",
    "coherence_score = coherence_model_lda.get_coherence()\n",
    "\n",
    "# Print coherence score\n",
    "print(\"Coherence Score:\", coherence_score)\n",
    "\n",
    "# Visualize the topics using pyLDAvis\n",
    "vis_data = pyLDAvis.gensim.prepare(lda_model, corpus_tfidf, id2word)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printout the average tags per questions in our data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the number of words in each row\n",
    "num_words_per_row = df_non_MLB['Tags'].apply(len)\n",
    "\n",
    "# Step 2: Calculate the average number of words\n",
    "average_words_per_row = num_words_per_row.mean()\n",
    "\n",
    "print(\"Average number of tags per question:\", round(average_words_per_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the LDA model by suggesting X tags per questions asked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = 0\n",
    "number_tag_suggested = 15\n",
    "\n",
    "input_text = df_LDA['Questions'][row_index]\n",
    "\n",
    "preprocessed_text_example  =  preprocessed_text(input_text)\n",
    "\n",
    "clean_input_text = gensim.utils.simple_preprocess(preprocessed_text_example, deacc=True)\n",
    "processed_input_text = trigram_mod[bigram_mod[clean_input_text]]\n",
    "\n",
    "input_bow = id2word.doc2bow(processed_input_text)\n",
    "\n",
    "input_topic_distribution = lda_model.get_document_topics(input_bow, minimum_probability=0.0)\n",
    "sorted_topics = sorted(input_topic_distribution, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "most_relevant_topic = sorted_topics[0][0]  # Select the most relevant topic\n",
    "second_relevant_topic = sorted_topics[1][0]  # Select the second most relevant topic [Ignoring the first one is better on avg]\n",
    "\n",
    "top_terms = lda_model.show_topic(most_relevant_topic, topn=number_tag_suggested)\n",
    "suggested_LDA_tags = [term[0] for term in top_terms]\n",
    "\n",
    "# if suggested_LDA_tags not in ['file', 'string', 'data', 'error', 'class', 'public', 'int', 'app', 'new', 'function', 'test', 'code', 'use', 'user', 'value']:\n",
    "#     top_terms = lda_model.show_topic(second_relevant_topic, topn=number_tag_suggested)\n",
    "#     suggested_LDA_tags = [term[0] for term in top_terms]\n",
    "# else:\n",
    "#     exit\n",
    "\n",
    "print(\"These are the\", number_tag_suggested, \"suggested tags for your question:\", suggested_LDA_tags)\n",
    "print(\"Real tags of that question:\", df_non_MLB['Tags'][row_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting j_score of the true and predicted values\n",
    "def j_score(y_true, y_pred):\n",
    "  jaccard = np.minimum(y_true, y_pred).sum(axis = 1)/np.maximum(y_true, y_pred).sum(axis = 1)\n",
    "  return jaccard.mean()*100\n",
    "\n",
    "# Printout the jaccard score of a model\n",
    "def print_score(y_pred, clf):\n",
    "  print(\"Model: \", clf.__class__.__name__)\n",
    "  print('Jaccard score: {}'.format(j_score(y_test, y_pred)))\n",
    "  print('----')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MLB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OG_TT['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df_MLB[['Questions']]\n",
    "y = df_MLB.drop('Questions', axis=1)\n",
    "titleT = df_OG_TT['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MLB.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "tfidf_test.fit(X['Questions'])\n",
    "X_TFIDF_test = tfidf_test.transform(X['Questions'])\n",
    "\n",
    "X_TFIDF_test.shape , y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_test , X_test_test , y_train  , y_test =  train_test_split (X_TFIDF_test , y , test_size = 0.2 , random_state=30 )\n",
    "X_train_test .shape  , X_test_test .shape  , y_train.shape   , y_test.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a first classifier between SGD , LR and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier()\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "svc = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier in [sgd, lr, svc]:\n",
    "  clf = OneVsRestClassifier(classifier)\n",
    "  clf.fit(X_train_test , y_train)\n",
    "  y_pred = clf.predict(X_test_test)\n",
    "  print_score(y_pred, classifier)\n",
    "  #52.63936479864412"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I) Searching for ngram_range optimal value with TFIDF fit on title and entire questions for L1 and L2 SVC models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) 1- L2 step on the entire question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_ranges = [(1, 1), (1, 2), (1, 3) , (2,2)]  # Different ngram_range values to iterate over\n",
    "\n",
    "for ngram_range in ngram_ranges:\n",
    "    tfidf = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=ngram_range)\n",
    "    tfidf.fit(X['Questions'])\n",
    "    \n",
    "    X_TFIDF = tfidf.transform(X['Questions'])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_TFIDF, y, test_size=0.2, random_state=30)\n",
    "    \n",
    "    print(f\"Processing ngram_range: {ngram_range}\")\n",
    "    \n",
    "    for classifier in [LinearSVC(penalty='l2', dual=True)]:\n",
    "        clf = OneVsRestClassifier(classifier)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print_score(y_pred, classifier)  # You need to define the print_score function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) 2- L2 step on the title only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_ranges = [(1, 1), (1, 2), (1, 3) , (2,2)]  # Different ngram_range values to iterate over\n",
    "\n",
    "for ngram_range in ngram_ranges:\n",
    "    tfidf = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=ngram_range)\n",
    "    tfidf.fit(titleT)\n",
    "    \n",
    "    X_TFIDF = tfidf.transform(X['Questions'])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_TFIDF, y, test_size=0.2, random_state=30)\n",
    "    \n",
    "    print(f\"Processing ngram_range: {ngram_range}\")\n",
    "    \n",
    "    for classifier in [LinearSVC(penalty='l2', dual=True)]:\n",
    "        clf = OneVsRestClassifier(classifier)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print_score(y_pred, classifier)  # You need to define the print_score function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) 1- L1 step on the entire question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_ranges = [(1, 1), (1, 2), (1, 3) , (2,2)]  # Different ngram_range values to iterate over\n",
    "\n",
    "for ngram_range in ngram_ranges:\n",
    "    tfidf = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=ngram_range)\n",
    "    tfidf.fit(X['Questions'])\n",
    "    \n",
    "    X_TFIDF = tfidf.transform(X['Questions'])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_TFIDF, y, test_size=0.2, random_state=30)\n",
    "    \n",
    "    print(f\"Processing ngram_range: {ngram_range}\")\n",
    "    \n",
    "    for classifier in [LinearSVC(penalty='l1', dual=False)]:\n",
    "        clf = OneVsRestClassifier(classifier)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print_score(y_pred, classifier)  # You need to define the print_score function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) 2- L1 step on the title only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_ranges = [(1, 1), (1, 2), (1, 3) , (2,2)]  # Different ngram_range values to iterate over\n",
    "\n",
    "for ngram_range in ngram_ranges:\n",
    "    tfidf = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=ngram_range)\n",
    "    tfidf.fit(titleT)\n",
    "    \n",
    "    X_TFIDF = tfidf.transform(X['Questions'])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_TFIDF, y, test_size=0.2, random_state=30)\n",
    "    \n",
    "    print(f\"Processing ngram_range: {ngram_range}\")\n",
    "    \n",
    "    for classifier in [LinearSVC(penalty='l1', dual=False)]:\n",
    "        clf = OneVsRestClassifier(classifier)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print_score(y_pred, classifier)  # You need to define the print_score function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC model optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II) Searching for C optimal value for SVC models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) TF IDF optimized for L1 SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word', stop_words='english' , ngram_range=(1,2))\n",
    "tfidf.fit(titleT)\n",
    "\n",
    "X_TFIDF = tfidf.transform(X['Questions'])\n",
    "\n",
    "X_train , X_test , y_train  , y_test =  train_test_split (X_TFIDF , y , test_size = 0.2 , random_state=30 )\n",
    "\n",
    "X_train.shape  , X_test.shape  , y_train.shape   , y_test.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating J score by iterating C paramter of SVC with L1 parameter (TFIDF  ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = np.arange(1, 10.5, 0.5)  # Create an array of C values from 1 to 10 with a step of 0.5\n",
    "jaccard_scores = []\n",
    "\n",
    "for C in C_values:\n",
    "    classifier = LinearSVC(C=C , penalty = 'l1' , dual=False)\n",
    "    clf = OneVsRestClassifier(classifier)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"For \", C , \"C  parameter value you  have these results\")\n",
    "    print_score(y_pred, classifier)\n",
    "    jaccard_score = j_score(y_test, y_pred)  # Replace with the actual Jaccard score calculation\n",
    "    jaccard_scores.append(jaccard_score)\n",
    "\n",
    "# Plot the Jaccard scores\n",
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(C_values, jaccard_scores , marker='o')\n",
    "plt.xlabel('C Parameter')\n",
    "plt.ylabel('Jaccard Score')\n",
    "plt.title('Jaccard Score vs C Parameter WITH L1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) TF IDF optimized for L2 SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word', stop_words='english' , ngram_range=(1,1))\n",
    "tfidf.fit(titleT)\n",
    "\n",
    "X_TFIDF = tfidf.transform(X['Questions'])\n",
    "\n",
    "X_train , X_test , y_train  , y_test =  train_test_split (X_TFIDF , y , test_size = 0.2 , random_state=30 )\n",
    "\n",
    "X_train.shape  , X_test.shape  , y_train.shape   , y_test.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating J score by iterating C paramter of SVC with L2 parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = np.arange(1, 10.5, 0.5)  # Create an array of C values from 1 to 10 with a step of 0.5\n",
    "jaccard_scores = []\n",
    "\n",
    "for C in C_values:\n",
    "    classifier = LinearSVC(C=C ,  penalty = 'l2' , dual=True)\n",
    "    clf = OneVsRestClassifier(classifier)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"For \", C , \"C  parameter value you  have these results\")\n",
    "    print_score(y_pred, classifier)\n",
    "    jaccard_score = j_score(y_test, y_pred)  # Replace with the actual Jaccard score calculation\n",
    "    jaccard_scores.append(jaccard_score)\n",
    "\n",
    "# Plot the Jaccard scores\n",
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(C_values, jaccard_scores , marker='o')\n",
    "plt.xlabel('C Parameter')\n",
    "plt.ylabel('Jaccard Score')\n",
    "plt.title('Jaccard Score vs C Parameter WITH L2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III) Final model  : TFIDF ngram_range=(1,2) , SVC (C=1.5 with L1) is the best option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word', stop_words=\"english\", ngram_range=(1, 2))\n",
    "tfidf.fit(titleT)\n",
    "X_TFIDF = tfidf.transform(X['Questions'])\n",
    "\n",
    "X_train , X_test , y_train  , y_test =  train_test_split (X_TFIDF , y , test_size = 0.2 , random_state=30 )\n",
    "\n",
    "X_train.shape  , X_test.shape  , y_train.shape   , y_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier in [LinearSVC(C=1.5 , penalty = 'l1' , dual=False) ]:\n",
    "  clf = OneVsRestClassifier(classifier)\n",
    "  clf.fit(X_train, y_train)\n",
    "  y_pred = clf.predict(X_test)\n",
    "  print_score(y_pred, classifier)\n",
    "  # 55,13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV) Testing the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test = 'How can I fine tune a BERT model ?'\n",
    "input_test_1 = 'How can I input an image into my website ?'\n",
    "input_test_2 = 'How to add interactivity with to my web site in a declarative way on the front end?'\n",
    "input_test_3 = 'What is the difference between high level and low level language programming ?'\n",
    "input_test_4 ='How to creat a boss script using c# in a video game ?'\n",
    "\n",
    "input_test_fail = 'What is the difference between supervised and unsupervised learning in the machine learning field, and what are their respective utilities?'\n",
    "\n",
    "test_text = input_test_2  # Take the first element of the list as a single string\n",
    "\n",
    "preprocessed_test = preprocessed_text(test_text)\n",
    "\n",
    "input_test_T = tfidf.transform([preprocessed_test])  # Pass the preprocessed text as a list\n",
    "\n",
    "predicted_df = pd.DataFrame(clf.predict(input_test_T), columns=y.columns)\n",
    "\n",
    "# Filter columns where there's only one 1 and print their names\n",
    "columns_with_single_1 = predicted_df.columns[predicted_df.apply(lambda col: col.sum() == 1)].tolist()\n",
    "\n",
    "print(\"The question is : \" , test_text)\n",
    "print(\"Here are the predicted tags:\")\n",
    "print(columns_with_single_1)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
