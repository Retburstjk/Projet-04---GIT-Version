{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import re\n",
    "import pyLDAvis.gensim\n",
    "import  numpy  as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import string\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOWER CASE DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def lower_case_df(text_to_lower):\n",
    "    if isinstance(text_to_lower, str):\n",
    "        tokens = word_tokenize(text_to_lower)\n",
    "        lowercased_tokens = [token.lower() for token in tokens]\n",
    "        return ' '.join(lowercased_tokens)\n",
    "    else:\n",
    "        return text_to_lower\n",
    "\n",
    "# Example usage:\n",
    "random_text = 'ThiS iS A tExT tO TEsT ThE fUnCTioN oF LOWerINg cAsE oF aLL LeTTErS'\n",
    "lowercased_string = lower_case_df(random_text)\n",
    "\n",
    "print(random_text)\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "print(lowercased_string)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVE HTLM SIGNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_htmlTags_punctuation(text):\n",
    "  \n",
    "    # Define the regular expression pattern for French punctuation marks\n",
    "    punctuation_pattern = r'[,?!;:…()\\[\\]«»—/\"\"\\'\\']'\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text_without_tags = soup.get_text()\n",
    "    \n",
    "    # Use re.sub() to remove the matched punctuation marks\n",
    "    cleaned_text = re.sub(punctuation_pattern, '', text_without_tags)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage:\n",
    "html_text = '<p>Voici un exemple de texte avec des ponctuations et des balises HTLM : «Salut !» et \\'Bonjour !\\'</p>'\n",
    "cleaned_text = remove_htmlTags_punctuation(html_text)\n",
    "print(html_text)\n",
    "print(\"--------------------------------------------------------------------------------------------------------\")\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEMMATIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(text):\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens \n",
    "              if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Sample text\n",
    "text = \"This is an example sentence that we'll use for text cleaning. It includes stopwords and different verb tenses.\"\n",
    "\n",
    "\n",
    "cleaned_text = lemmatizer(text)\n",
    "\n",
    "print(text)\n",
    "print(\"------------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL PREPROCESS FUNCTIONS INTO ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def preprocessed_text(text):\n",
    "    # Define the regular expression pattern for French punctuation marks\n",
    "    punctuation_pattern = r'[,?!;:…()\\[\\]«»—/\"\"\\'\\']'\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text_without_tags = soup.get_text()\n",
    "    \n",
    "    # Use re.sub() to remove the matched punctuation marks\n",
    "    cleaned_text = re.sub(punctuation_pattern, '', text_without_tags)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(cleaned_text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens \n",
    "              if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Sample text with HTML tags and punctuation\n",
    "sample_text = \"\"\"\n",
    "<html>\n",
    "<head><title>Sample Text</title></head>\n",
    "<body>\n",
    "<p>This is an example of text with HTML tags, punctuation, and some stop words. It's a sample text for preprocessing.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Call the preprocessed_text function to preprocess the sample text\n",
    "preprocessed_sample = preprocessed_text(sample_text)\n",
    "\n",
    "print(sample_text)\n",
    "print(\"------------------------------------------------------------------------------------------------------------------------\")\n",
    "# Display the preprocessed text\n",
    "print(preprocessed_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a MLB Dataframe with the top X most frequents tags with atleast a 1 in the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_by_tags(df, all_tags, X):\n",
    "    # Step 4: Take the first X tags from the list\n",
    "    first_tags = all_tags[:X]\n",
    "\n",
    "    # Step 5: Filter the DataFrame based on the first X tags\n",
    "    filtered_df = df[df[first_tags].any(axis=1)]\n",
    "\n",
    "    # Step 6: Drop all columns beyond the first X tags\n",
    "    filtered_df = filtered_df.drop(columns=all_tags[X:])\n",
    "    filtered_df = pd.DataFrame(filtered_df)\n",
    "    \n",
    "    return filtered_df\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
