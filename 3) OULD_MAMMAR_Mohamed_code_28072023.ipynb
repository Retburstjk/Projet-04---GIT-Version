{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import re\n",
    "import pyLDAvis.gensim\n",
    "import  numpy  as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import string\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOWER CASE DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def lower_case_df(text_to_lower):\n",
    "    if isinstance(text_to_lower, str):\n",
    "        tokens = word_tokenize(text_to_lower)\n",
    "        lowercased_tokens = [token.lower() for token in tokens]\n",
    "        return ' '.join(lowercased_tokens)\n",
    "    else:\n",
    "        return text_to_lower\n",
    "\n",
    "# Example usage:\n",
    "random_text = 'ThiS iS A tExT tO TEsT ThE fUnCTioN oF LOWerINg cAsE oF aLL LeTTErS'\n",
    "lowercased_string = lower_case_df(random_text)\n",
    "\n",
    "print(random_text)\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "print(lowercased_string)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVE HTLM SIGNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_htmlTags_punctuation(text):\n",
    "  \n",
    "    # Define the regular expression pattern for French punctuation marks\n",
    "    punctuation_pattern = r'[,?!;:…()\\[\\]«»—/\"\"\\'\\']'\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text_without_tags = soup.get_text()\n",
    "    \n",
    "    # Use re.sub() to remove the matched punctuation marks\n",
    "    cleaned_text = re.sub(punctuation_pattern, '', text_without_tags)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage:\n",
    "html_text = '<p>Voici un exemple de texte avec des ponctuations et des balises HTLM : «Salut !» et \\'Bonjour !\\'</p>'\n",
    "cleaned_text = remove_htmlTags_punctuation(html_text)\n",
    "print(html_text)\n",
    "print(\"--------------------------------------------------------------------------------------------------------\")\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEMMATIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(text):\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens \n",
    "              if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Sample text\n",
    "text = \"This is an example sentence that we'll use for text cleaning. It includes stopwords and different verb tenses.\"\n",
    "\n",
    "\n",
    "cleaned_text = lemmatizer(text)\n",
    "\n",
    "print(text)\n",
    "print(\"------------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL PREPROCESS FUNCTIONS INTO ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def preprocessed_text(text):\n",
    "    # Define the regular expression pattern for French punctuation marks\n",
    "    punctuation_pattern = r'[,?!;:…()\\[\\]«»—/\"\"\\'\\']'\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text_without_tags = soup.get_text()\n",
    "    \n",
    "    # Use re.sub() to remove the matched punctuation marks\n",
    "    cleaned_text = re.sub(punctuation_pattern, '', text_without_tags)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(cleaned_text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens \n",
    "              if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Sample text with HTML tags and punctuation\n",
    "sample_text = \"\"\"\n",
    "<html>\n",
    "<head><title>Sample Text</title></head>\n",
    "<body>\n",
    "<p>This is an example of text with HTML tags, punctuation, and some stop words. It's a sample text for preprocessing.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Call the preprocessed_text function to preprocess the sample text\n",
    "preprocessed_sample = preprocessed_text(sample_text)\n",
    "\n",
    "print(sample_text)\n",
    "print(\"------------------------------------------------------------------------------------------------------------------------\")\n",
    "# Display the preprocessed text\n",
    "print(preprocessed_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a MLB Dataframe with the top X most frequents tags with atleast a 1 in the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_by_tags(df, all_tags, X):\n",
    "    # Step 4: Take the first X tags from the list\n",
    "    first_tags = all_tags[:X]\n",
    "\n",
    "    # Step 5: Filter the DataFrame based on the first X tags\n",
    "    filtered_df = df[df[first_tags].any(axis=1)]\n",
    "\n",
    "    # Step 6: Drop all columns beyond the first X tags\n",
    "    filtered_df = filtered_df.drop(columns=all_tags[X:])\n",
    "    filtered_df = pd.DataFrame(filtered_df)\n",
    "    \n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reading / Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DF_OG = pd.read_csv(\"DF_OG.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_OG.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_OG.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_OG.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tags from each row using regular expression\n",
    "tags_list = [re.findall(r'<(.*?)>', row) for row in DF_OG['Tags']]\n",
    "\n",
    "combined_tags = [tag for sublist in tags_list for tag in sublist]\n",
    "\n",
    "# Convert the list into a set to retain unique tags\n",
    "unique_tags = set(combined_tags)\n",
    "\n",
    "unique_tags_array = np.array(unique_tags)\n",
    "\n",
    "unique_tags_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csharp_rows = DF_OG[DF_OG['Body'].str.contains('c#', case=False)]\n",
    "csharp_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lemmatizer = pd.DataFrame([])\n",
    "test_lemmatizer['Title'] = DF_OG['Title'].apply(remove_htmlTags_punctuation)\n",
    "test_lemmatizer['Body'] = DF_OG['Body'].apply(remove_htmlTags_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csharp_rows = test_lemmatizer[test_lemmatizer['Body'].str.contains('c#', case=False)]\n",
    "csharp_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_lemmatizer['Title'] = test_lemmatizer['Title'].apply(lemmatizer)\n",
    "test_lemmatizer['Body'] = test_lemmatizer['Body'].apply(lemmatizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csharp_rows = test_lemmatizer[test_lemmatizer['Body'].str.contains('c #', case=False)]\n",
    "csharp_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(np.log(DF_OG['ViewCount']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_OG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the removing HTML signs to the 'Questions' column\n",
    "DF_OG['Title'] = DF_OG['Title'].apply(preprocessed_text)\n",
    "DF_OG['Body'] = DF_OG['Body'].apply(preprocessed_text)\n",
    "\n",
    "\n",
    "df_MultiLabel = DF_OG[['Title' , 'Body' , 'Tags']]\n",
    "\n",
    "df_MultiLabel['Questions'] = df_MultiLabel['Title'] + \" \" + df_MultiLabel['Body']\n",
    "# df_MultiLabel['Questions'] = df_MultiLabel['Questions'].apply(remove_html_tags)\n",
    "\n",
    "\n",
    "df_MultiLabel = df_MultiLabel.drop(['Title' , 'Body'] , axis=1)\n",
    "# Swap the positions of columns Tags and Questions\n",
    "df_MultiLabel['Tags'], df_MultiLabel['Questions'] = df_MultiLabel['Questions'], df_MultiLabel['Tags'].copy()\n",
    "# Swap the positions and column names of columns B and C\n",
    "df_MultiLabel = df_MultiLabel.rename(columns={'Tags': 'Questions', \n",
    "                                  'Questions': 'Tags'})\n",
    "\n",
    "df_MultiLabel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"c #\" with \"c#\"\n",
    "df_MultiLabel['Questions'] = df_MultiLabel['Questions'].str.replace('c #', 'c#')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tags from each row using regular expression\n",
    "tags_list = [re.findall(r'<(.*?)>', row) for row in df_MultiLabel['Tags']]\n",
    "\n",
    "# Combine all tags into a single list\n",
    "combined_tags = [tag for sublist in tags_list for tag in sublist]\n",
    "\n",
    "# Convert the list into a set to retain unique tags\n",
    "unique_tags = set(combined_tags)\n",
    "\n",
    "# Count the number of unique tags\n",
    "num_unique_tags = len(unique_tags)\n",
    "\n",
    "print(\"Number of unique tags:\", num_unique_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each tag\n",
    "tag_counts = {}\n",
    "for tag in combined_tags:\n",
    "    if tag in tag_counts:\n",
    "        tag_counts[tag] += 1\n",
    "    else:\n",
    "        tag_counts[tag] = 1\n",
    "\n",
    "# Sort the tags based on their counts in descending order\n",
    "sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Repeated tags (sorted by count) FORMAT => TAG : COUNT:\")\n",
    "for tag, count in sorted_tags:\n",
    "    print(f\"Tag: {tag} : {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each tag\n",
    "tag_counts = pd.Series(combined_tags).value_counts()\n",
    "\n",
    "# Sort the counts in descending order\n",
    "sorted_counts = tag_counts.sort_values(ascending=False)\n",
    "\n",
    "# Calculate the sum of the repeated numbers\n",
    "sum_repeated_numbers = sorted_counts.sum()\n",
    "\n",
    "print(\"Sum of repeated numbers:\", sum_repeated_numbers)\n",
    "\n",
    "# Generate normal numbers for x-axis\n",
    "x_values = list(range(1, len(sorted_counts) + 1))\n",
    "\n",
    "# Plot the graph using sns.histplot\n",
    "sns.histplot(x=x_values, bins='auto', weights=sorted_counts.values, kde=False, color='b')\n",
    "plt.xlabel('Number')\n",
    "plt.ylabel('Repetition Count')\n",
    "plt.title('Repetition Count Histogram')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_values = pd.DataFrame(sorted_tags)[1]\n",
    "\n",
    "# Slice the list to take the first 100 values\n",
    "first_100_values = repeated_values[:100]\n",
    "\n",
    "# Create a plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(first_100_values) + 1), first_100_values)\n",
    "\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Graph of First 100 Repeated Values')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MultiLabel['Tags'] = df_MultiLabel['Tags'].str.findall(r'<(.*?)>')\n",
    "\n",
    "# df_MultiLabel['Questions'] = df_MultiLabel['Questions'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "all_tags = pd.DataFrame(sorted_tags)[0]\n",
    "\n",
    "df_non_MLB =  df_MultiLabel\n",
    "df_non_MLB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Labeling the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLB = MultiLabelBinarizer(classes=all_tags)\n",
    "tags_binary = MLB.fit_transform(df_MultiLabel['Tags'])\n",
    "\n",
    "# Convert the binary representation to a DataFrame\n",
    "tags_df = pd.DataFrame(tags_binary, columns=MLB.classes_)\n",
    "\n",
    "# Step 3: Concatenate the binary representation to the original DataFrame\n",
    "df_MultiLabel = pd.concat([df_MultiLabel, tags_df], axis=1)\n",
    "df_MultiLabel.drop('Tags' , axis=1 , inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the value of how many tags to have 90% of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_targeted_value = 90\n",
    "\n",
    "filtered_rows_percent = []  # List to store filtered_df.shape[0] values in percentage\n",
    "\n",
    "for labels_number in range(101):\n",
    "    filtered_df = filter_by_tags(df_MultiLabel, all_tags, labels_number)\n",
    "    percent = (filtered_df.shape[0] / DF_OG.shape[0]) * 100  # Convert to percentage\n",
    "    filtered_rows_percent.append(percent)\n",
    "\n",
    "# Find the index where the Y value is closest to 90%\n",
    "index_90_percent = filtered_rows_percent.index(min(filtered_rows_percent, key=lambda x: abs(x - percentage_targeted_value)))\n",
    "x_90_percent = index_90_percent  # X value where Y is closest to 90%\n",
    "y_90_percent = filtered_rows_percent[index_90_percent]  # Y value closest to 90%\n",
    "\n",
    "# Create a plot with  90% threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(101), filtered_rows_percent, marker='o')\n",
    "\n",
    "plt.axhline(y=percentage_targeted_value, color='g', linestyle='--', label='90% Threshold')\n",
    "plt.axvline(x=x_90_percent, color='g', linestyle='--', label=f'X at {x_90_percent}')\n",
    "plt.annotate(f'({x_90_percent}, {y_90_percent:.2f}%)', (x_90_percent, y_90_percent + 2), color='g', xytext=(x_90_percent + 1, y_90_percent + 4))\n",
    "\n",
    "plt.xlabel('Number of Tags')\n",
    "plt.ylabel('Taille du data frame (%)')\n",
    "plt.title('Nombre de Tags en feature par rapport a la taille original (%)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_number = 60\n",
    "#Takign top 60 tags where there is atleast 1 on those tags\n",
    "filtered_df = filter_by_tags(df_MultiLabel  , all_tags , labels_number)\n",
    "\n",
    "# Sample X % of the data (you can adjust the fraction as needed)\n",
    "sample_df = filtered_df.sample(frac=1, random_state=42)\n",
    "df_MLB = sample_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop the \"Questions\" column\n",
    "sample_df_without_questions = df_MLB.drop(columns=[\"Questions\"])\n",
    "\n",
    "# Sum of feature values excluding the \"Questions\" column\n",
    "sum_of_features = sample_df_without_questions.sum()\n",
    "\n",
    "# Check if any sum is equal to 0\n",
    "any_zero_value = (sum_of_features == 0).any()\n",
    "\n",
    "print(\"Sum of feature values:\")\n",
    "print(sum_of_features)\n",
    "print(\"\\nAre there any sums equal to 0?\")\n",
    "print(any_zero_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_display = sample_df.index.tolist()  # Get the list of index values from sample_df\n",
    "\n",
    "# Create a new DataFrame containing the rows with the specified index values from df_non_MLB\n",
    "df_non_MLB = df_non_MLB.loc[indices_to_display]\n",
    "df_non_MLB = df_non_MLB.sort_index()\n",
    "\n",
    "df_OG_TT = DF_OG[['Title' , 'Body' , 'Tags']]\n",
    "df_OG_TT = df_OG_TT.loc[indices_to_display]\n",
    "df_OG_TT = df_OG_TT.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OG_TT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_MLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csharp_rows = df_non_MLB[df_non_MLB['Questions'].str.contains('c#', case=False)]\n",
    "csharp_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OG_TT['Tags'] = df_OG_TT['Tags'].str.findall(r'<(.*?)>')\n",
    "df_OG_TT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MLB.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import gensim\n",
    "\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import corpora, models\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import ast\n",
    "import  numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "import string\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process the text data\n",
    "def process_text(texts):\n",
    "    \n",
    "    bigram = Phrases(texts, min_count=5, threshold=100)\n",
    "    trigram = Phrases(bigram[texts], min_count=5, threshold=100)\n",
    "\n",
    "    # Apply the bigram and trigram models\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    trigram_mod = Phraser(trigram)\n",
    "    \n",
    "    # Apply bigram and trigram models\n",
    "    texts_P = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = gensim.parsing.preprocessing.STOPWORDS\n",
    "    texts_F = [[word for word in doc if word not in stop_words] for doc in texts_P]\n",
    "    \n",
    "    return texts_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed_text(text):\n",
    "    # Define the regular expression pattern for French punctuation marks\n",
    "    punctuation_pattern = r'[,?!;:…()\\[\\]«»—/\"\"\\'\\']'\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text_without_tags = soup.get_text()\n",
    "    \n",
    "    # Use re.sub() to remove the matched punctuation marks\n",
    "    cleaned_text = re.sub(punctuation_pattern, '', text_without_tags)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(cleaned_text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens \n",
    "              if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre building of the LDA model with bi/trigram  phraser and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LDA = df_non_MLB[['Questions']]\n",
    "\n",
    "# df_LDA['Questions'] =  df_LDA['Questions'].apply(clean_text)\n",
    "# Preprocess the text data\n",
    "df_LDA['text'] = df_LDA['Questions']\n",
    "df_LDA['clean_text'] = df_LDA['text'].apply(gensim.utils.simple_preprocess, deacc=True)\n",
    "\n",
    "# Create bigrams and trigrams\n",
    "text_data = df_LDA['clean_text'].tolist()\n",
    "\n",
    "bigram = Phrases(text_data, min_count=5, threshold=100)\n",
    "trigram = Phrases(bigram[text_data], min_count=5, threshold=100)\n",
    "\n",
    "# Apply the bigram and trigram models\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)\n",
    "\n",
    "# Apply bigram and trigram models\n",
    "texts_P = [trigram_mod[bigram_mod[doc]] for doc in text_data]\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = gensim.parsing.preprocessing.STOPWORDS\n",
    "texts_F = [[word for word in doc if word not in stop_words] for doc in texts_P]\n",
    "\n",
    "# Create the dictionary\n",
    "id2word = corpora.Dictionary(texts_F)\n",
    "\n",
    "# Create the corpus with TF-IDF representation\n",
    "texts = [id2word.doc2bow(doc) for doc in texts_F]\n",
    "\n",
    "tfidf = models.TfidfModel(texts)\n",
    "corpus_tfidf = tfidf[texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_word = \"c#\"\n",
    "# count = sum(1 for word in processed_data if search_word in word)\n",
    "\n",
    "# print(\"Number of occurrences of '\",search_word ,\" ':\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating  the  coherence score iterating the num-topics parameter  of the  LDA model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_scores = []\n",
    "num_topics_range = range(1, 100, 1)  # Range of num_topics values from 2 to 60 jumping by 2\n",
    "\n",
    "for num_topics in num_topics_range:\n",
    "    # Train the LDA model (using LdaMulticore)\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus_tfidf,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics,\n",
    "                                           random_state=100,\n",
    "                                           passes=10,\n",
    "                                           workers=4)\n",
    "\n",
    "    # Compute coherence score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts_F, dictionary=id2word, coherence='c_v')\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    coherence_scores.append(coherence_score)\n",
    "\n",
    "    print(\"num_topics:\", num_topics, \"Coherence Score:\", coherence_score)\n",
    "\n",
    "\n",
    "# Plot the coherence scores against alpha values\n",
    "plt.figure(figsize=(15, 9))\n",
    "# Plot the coherence scores\n",
    "plt.plot(num_topics_range, coherence_scores, marker='o')\n",
    "plt.xlabel(\"num_topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.title(\"Coherence Score vs num_topics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating  the  coherence score iterating the alpha parameter  of the  LDA model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of alpha values to test\n",
    "alpha_values = np.arange(0.1, 0.51, 0.01)\n",
    "\n",
    "# List to store coherence scores for each alpha\n",
    "coherence_scores = []\n",
    "\n",
    "# Train LDA models and calculate coherence scores\n",
    "for alpha in alpha_values:\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus_tfidf,\n",
    "                                           id2word=id2word,\n",
    "                                           alpha=alpha,\n",
    "                                           num_topics=41,  # Adjust the number of topics as desired\n",
    "                                           random_state=100,\n",
    "                                           passes=10,\n",
    "                                           workers=4)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts_F, dictionary=id2word, coherence='c_v')\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    coherence_scores.append(coherence_score)\n",
    "    \n",
    "    print(f\"Alpha: {alpha:.2f} - Coherence Score: {coherence_score:.4f}\")\n",
    "\n",
    "# Plot the coherence scores against alpha values\n",
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(alpha_values, coherence_scores, marker='o')\n",
    "plt.xlabel('Alpha Value')\n",
    "plt.ylabel('Coherence Score')\n",
    "plt.title('LDA Model Coherence Score vs. Alpha Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the LDA model with optimized parameters (alpha=0.43   , num_topics=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LDA model (using LdaMulticore)\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus_tfidf,\n",
    "                                       id2word=id2word,\n",
    "                                       alpha=0.43,\n",
    "                                       num_topics=41,  # Adjust the number of topics as desired\n",
    "                                       random_state=100,\n",
    "                                       passes=10,\n",
    "                                       workers=4)\n",
    "\n",
    "# Compute coherence score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts_F, dictionary=id2word, coherence='c_v')\n",
    "coherence_score = coherence_model_lda.get_coherence()\n",
    "\n",
    "# Print coherence score\n",
    "print(\"Coherence Score:\", coherence_score)\n",
    "\n",
    "# Visualize the topics using pyLDAvis\n",
    "vis_data = pyLDAvis.gensim.prepare(lda_model, corpus_tfidf, id2word)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printout the average tags per questions in our data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the number of words in each row\n",
    "num_words_per_row = df_non_MLB['Tags'].apply(len)\n",
    "\n",
    "# Step 2: Calculate the average number of words\n",
    "average_words_per_row = num_words_per_row.mean()\n",
    "\n",
    "print(\"Average number of tags per question:\", round(average_words_per_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the LDA model by suggesting X tags per questions asked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = 0\n",
    "number_tag_suggested = 15\n",
    "\n",
    "input_text = df_LDA['Questions'][row_index]\n",
    "\n",
    "preprocessed_text_example  =  preprocessed_text(input_text)\n",
    "\n",
    "clean_input_text = gensim.utils.simple_preprocess(preprocessed_text_example, deacc=True)\n",
    "processed_input_text = trigram_mod[bigram_mod[clean_input_text]]\n",
    "\n",
    "input_bow = id2word.doc2bow(processed_input_text)\n",
    "\n",
    "input_topic_distribution = lda_model.get_document_topics(input_bow, minimum_probability=0.0)\n",
    "sorted_topics = sorted(input_topic_distribution, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "most_relevant_topic = sorted_topics[0][0]  # Select the most relevant topic\n",
    "second_relevant_topic = sorted_topics[1][0]  # Select the second most relevant topic [Ignoring the first one is better on avg]\n",
    "\n",
    "top_terms = lda_model.show_topic(most_relevant_topic, topn=number_tag_suggested)\n",
    "suggested_LDA_tags = [term[0] for term in top_terms]\n",
    "\n",
    "# if suggested_LDA_tags not in ['file', 'string', 'data', 'error', 'class', 'public', 'int', 'app', 'new', 'function', 'test', 'code', 'use', 'user', 'value']:\n",
    "#     top_terms = lda_model.show_topic(second_relevant_topic, topn=number_tag_suggested)\n",
    "#     suggested_LDA_tags = [term[0] for term in top_terms]\n",
    "# else:\n",
    "#     exit\n",
    "\n",
    "print(\"These are the\", number_tag_suggested, \"suggested tags for your question:\", suggested_LDA_tags)\n",
    "print(\"Real tags of that question:\", df_non_MLB['Tags'][row_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting j_score of the true and predicted values\n",
    "def j_score(y_true, y_pred):\n",
    "  jaccard = np.minimum(y_true, y_pred).sum(axis = 1)/np.maximum(y_true, y_pred).sum(axis = 1)\n",
    "  return jaccard.mean()*100\n",
    "\n",
    "# Printout the jaccard score of a model\n",
    "def print_score(y_pred, clf):\n",
    "  print(\"Model: \", clf.__class__.__name__)\n",
    "  print('Jaccard score: {}'.format(j_score(y_test, y_pred)))\n",
    "  print('----')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MLB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OG_TT['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df_MLB[['Questions']]\n",
    "y = df_MLB.drop('Questions', axis=1)\n",
    "titleT = df_OG_TT['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MLB.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "tfidf_test.fit(X['Questions'])\n",
    "X_TFIDF_test = tfidf_test.transform(X['Questions'])\n",
    "\n",
    "X_TFIDF_test.shape , y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_test , X_test_test , y_train  , y_test =  train_test_split (X_TFIDF_test , y , test_size = 0.2 , random_state=30 )\n",
    "X_train_test .shape  , X_test_test .shape  , y_train.shape   , y_test.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a first classifier between SGD , LR and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier()\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "svc = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier in [sgd, lr, svc]:\n",
    "  clf = OneVsRestClassifier(classifier)\n",
    "  clf.fit(X_train_test , y_train)\n",
    "  y_pred = clf.predict(X_test_test)\n",
    "  print_score(y_pred, classifier)\n",
    "  #52.63936479864412"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I) Searching for ngram_range optimal value with TFIDF fit on title and entire questions for L1 and L2 SVC models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) 1- L2 step on the entire question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_ranges = [(1, 1), (1, 2), (1, 3) , (2,2)]  # Different ngram_range values to iterate over\n",
    "\n",
    "for ngram_range in ngram_ranges:\n",
    "    tfidf = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=ngram_range)\n",
    "    tfidf.fit(X['Questions'])\n",
    "    \n",
    "    X_TFIDF = tfidf.transform(X['Questions'])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_TFIDF, y, test_size=0.2, random_state=30)\n",
    "    \n",
    "    print(f\"Processing ngram_range: {ngram_range}\")\n",
    "    \n",
    "    for classifier in [LinearSVC(penalty='l2', dual=True)]:\n",
    "        clf = OneVsRestClassifier(classifier)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print_score(y_pred, classifier)  # You need to define the print_score function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) 2- L2 step on the title only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_ranges = [(1, 1), (1, 2), (1, 3) , (2,2)]  # Different ngram_range values to iterate over\n",
    "\n",
    "for ngram_range in ngram_ranges:\n",
    "    tfidf = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=ngram_range)\n",
    "    tfidf.fit(titleT)\n",
    "    \n",
    "    X_TFIDF = tfidf.transform(X['Questions'])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_TFIDF, y, test_size=0.2, random_state=30)\n",
    "    \n",
    "    print(f\"Processing ngram_range: {ngram_range}\")\n",
    "    \n",
    "    for classifier in [LinearSVC(penalty='l2', dual=True)]:\n",
    "        clf = OneVsRestClassifier(classifier)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print_score(y_pred, classifier)  # You need to define the print_score function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) 1- L1 step on the entire question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_ranges = [(1, 1), (1, 2), (1, 3) , (2,2)]  # Different ngram_range values to iterate over\n",
    "\n",
    "for ngram_range in ngram_ranges:\n",
    "    tfidf = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=ngram_range)\n",
    "    tfidf.fit(X['Questions'])\n",
    "    \n",
    "    X_TFIDF = tfidf.transform(X['Questions'])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_TFIDF, y, test_size=0.2, random_state=30)\n",
    "    \n",
    "    print(f\"Processing ngram_range: {ngram_range}\")\n",
    "    \n",
    "    for classifier in [LinearSVC(penalty='l1', dual=False)]:\n",
    "        clf = OneVsRestClassifier(classifier)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print_score(y_pred, classifier)  # You need to define the print_score function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) 2- L1 step on the title only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_ranges = [(1, 1), (1, 2), (1, 3) , (2,2)]  # Different ngram_range values to iterate over\n",
    "\n",
    "for ngram_range in ngram_ranges:\n",
    "    tfidf = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=ngram_range)\n",
    "    tfidf.fit(titleT)\n",
    "    \n",
    "    X_TFIDF = tfidf.transform(X['Questions'])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_TFIDF, y, test_size=0.2, random_state=30)\n",
    "    \n",
    "    print(f\"Processing ngram_range: {ngram_range}\")\n",
    "    \n",
    "    for classifier in [LinearSVC(penalty='l1', dual=False)]:\n",
    "        clf = OneVsRestClassifier(classifier)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print_score(y_pred, classifier)  # You need to define the print_score function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC model optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II) Searching for C optimal value for SVC models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) TF IDF optimized for L1 SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word', stop_words='english' , ngram_range=(1,2))\n",
    "tfidf.fit(titleT)\n",
    "\n",
    "X_TFIDF = tfidf.transform(X['Questions'])\n",
    "\n",
    "X_train , X_test , y_train  , y_test =  train_test_split (X_TFIDF , y , test_size = 0.2 , random_state=30 )\n",
    "\n",
    "X_train.shape  , X_test.shape  , y_train.shape   , y_test.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating J score by iterating C paramter of SVC with L1 parameter (TFIDF  ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = np.arange(1, 10.5, 0.5)  # Create an array of C values from 1 to 10 with a step of 0.5\n",
    "jaccard_scores = []\n",
    "\n",
    "for C in C_values:\n",
    "    classifier = LinearSVC(C=C , penalty = 'l1' , dual=False)\n",
    "    clf = OneVsRestClassifier(classifier)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"For \", C , \"C  parameter value you  have these results\")\n",
    "    print_score(y_pred, classifier)\n",
    "    jaccard_score = j_score(y_test, y_pred)  # Replace with the actual Jaccard score calculation\n",
    "    jaccard_scores.append(jaccard_score)\n",
    "\n",
    "# Plot the Jaccard scores\n",
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(C_values, jaccard_scores , marker='o')\n",
    "plt.xlabel('C Parameter')\n",
    "plt.ylabel('Jaccard Score')\n",
    "plt.title('Jaccard Score vs C Parameter WITH L1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) TF IDF optimized for L2 SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word', stop_words='english' , ngram_range=(1,1))\n",
    "tfidf.fit(titleT)\n",
    "\n",
    "X_TFIDF = tfidf.transform(X['Questions'])\n",
    "\n",
    "X_train , X_test , y_train  , y_test =  train_test_split (X_TFIDF , y , test_size = 0.2 , random_state=30 )\n",
    "\n",
    "X_train.shape  , X_test.shape  , y_train.shape   , y_test.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating J score by iterating C paramter of SVC with L2 parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = np.arange(1, 10.5, 0.5)  # Create an array of C values from 1 to 10 with a step of 0.5\n",
    "jaccard_scores = []\n",
    "\n",
    "for C in C_values:\n",
    "    classifier = LinearSVC(C=C ,  penalty = 'l2' , dual=True)\n",
    "    clf = OneVsRestClassifier(classifier)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"For \", C , \"C  parameter value you  have these results\")\n",
    "    print_score(y_pred, classifier)\n",
    "    jaccard_score = j_score(y_test, y_pred)  # Replace with the actual Jaccard score calculation\n",
    "    jaccard_scores.append(jaccard_score)\n",
    "\n",
    "# Plot the Jaccard scores\n",
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(C_values, jaccard_scores , marker='o')\n",
    "plt.xlabel('C Parameter')\n",
    "plt.ylabel('Jaccard Score')\n",
    "plt.title('Jaccard Score vs C Parameter WITH L2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III) Final model  : TFIDF ngram_range=(1,2) , SVC (C=1.5 with L1) is the best option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word', stop_words=\"english\", ngram_range=(1, 2))\n",
    "tfidf.fit(titleT)\n",
    "X_TFIDF = tfidf.transform(X['Questions'])\n",
    "\n",
    "X_train , X_test , y_train  , y_test =  train_test_split (X_TFIDF , y , test_size = 0.2 , random_state=30 )\n",
    "\n",
    "X_train.shape  , X_test.shape  , y_train.shape   , y_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier in [LinearSVC(C=1.5 , penalty = 'l1' , dual=False) ]:\n",
    "  clf = OneVsRestClassifier(classifier)\n",
    "  clf.fit(X_train, y_train)\n",
    "  y_pred = clf.predict(X_test)\n",
    "  print_score(y_pred, classifier)\n",
    "  # 55,13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV) Testing the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test = 'How can I fine tune a BERT model ?'\n",
    "input_test_1 = 'How can I input an image into my website ?'\n",
    "input_test_2 = 'How to add interactivity with to my web site in a declarative way on the front end?'\n",
    "input_test_3 = 'What is the difference between high level and low level language programming ?'\n",
    "input_test_4 ='How to creat a boss script using c# in a video game ?'\n",
    "\n",
    "input_test_fail = 'What is the difference between supervised and unsupervised learning in the machine learning field, and what are their respective utilities?'\n",
    "\n",
    "test_text = input_test_2  # Take the first element of the list as a single string\n",
    "\n",
    "preprocessed_test = preprocessed_text(test_text)\n",
    "\n",
    "input_test_T = tfidf.transform([preprocessed_test])  # Pass the preprocessed text as a list\n",
    "\n",
    "predicted_df = pd.DataFrame(clf.predict(input_test_T), columns=y.columns)\n",
    "\n",
    "# Filter columns where there's only one 1 and print their names\n",
    "columns_with_single_1 = predicted_df.columns[predicted_df.apply(lambda col: col.sum() == 1)].tolist()\n",
    "\n",
    "print(\"The question is : \" , test_text)\n",
    "print(\"Here are the predicted tags:\")\n",
    "print(columns_with_single_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning (BERT fine-tunning) using PYTORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.*\")\n",
    "import os\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, AutoModelForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau , CosineAnnealingLR\n",
    "from sklearn.metrics import f1_score, jaccard_score\n",
    "import ast\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy\n",
    "\n",
    "import random\n",
    "\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model  evaluation function (with freezing all  layers as  an option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model, data_loader, loss_fn, freeze_all=True):\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = freeze_all\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total_correct_test = 0\n",
    "    total_samples_test = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    predictions = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad(), tqdm(total=len(data_loader), desc=\"Testing\") as pbar:\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            loss = loss_fn(outputs.logits, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predicted_probs = torch.sigmoid(outputs.logits)\n",
    "            predicted_tags = (predicted_probs >= 0.5).float()\n",
    "\n",
    "            total_correct_test += torch.sum(torch.eq(predicted_tags, targets)).item()\n",
    "            total_samples_test += len(targets)\n",
    "\n",
    "            predictions.append(predicted_tags.cpu().numpy())\n",
    "            targets_list.append(targets.cpu().numpy())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    avg_test_accuracy = total_correct_test / total_samples_test\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    predictions = np.concatenate(predictions)\n",
    "    targets_list = np.concatenate(targets_list)\n",
    "\n",
    "    avg_f1_micro = f1_score(targets_list, predictions, average='micro')\n",
    "    avg_jaccard_micro = jaccard_score(targets_list, predictions, average='micro')\n",
    "\n",
    "    avg_f1_macro = f1_score(targets_list, predictions, average='macro')\n",
    "    avg_jaccard_macro = jaccard_score(targets_list, predictions, average='macro')\n",
    "  \n",
    "    print(\"------------------------------------- Evaluation DONE ! --------------------------------------\")\n",
    "    print(f\"Test Accuracy: {avg_test_accuracy:.3f}\")\n",
    "    print(f\"Average Loss: {avg_loss:.5f}\")\n",
    "    print(f\"Micro-average F1 Score: {avg_f1_micro:.4f} - Macro-average F1 Score: {avg_f1_macro:.4f}\")\n",
    "    print(f\"Micro-average Jaccard Score: {avg_jaccard_micro:.4f}  - Macro-average Jaccard Score: {avg_jaccard_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best learning rate finder (with elbow method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_best_finder(model, data_loader, loss_fn, lr_values, sample_size = 0.1 , lr_finder=False ):\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Create a deep copy of the model\n",
    "    edited_model = copy.deepcopy(model)\n",
    "\n",
    "    edited_model.train()\n",
    "    output_layer_index = -1  # Layers index's not  to freeze (-1  means  the  last one)\n",
    "\n",
    "    for idx, param in enumerate(edited_model.parameters()):  # Freeze all layers except the output_layer_index (last one)\n",
    "        if idx < output_layer_index:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "\n",
    "    edited_model.to(device)\n",
    "    loss_fn.to(device)\n",
    "\n",
    "    batch = next(iter(data_loader))\n",
    "    losses = []\n",
    "\n",
    "    for lr in lr_values:\n",
    "        optimizer = optim.AdamW(edited_model.parameters(), lr=lr)\n",
    "        total_loss = 0.0\n",
    "        num_batches = int(len(data_loader) * sample_size)\n",
    "\n",
    "        for iteration in tqdm(range(num_batches), desc=f\"Training for LR {lr:.2e}\"):\n",
    "            \n",
    "            # Create a random subset of the data for each iteration to guarantee a good generalization with shuffling and randomness\n",
    "            shuffled_indices = list(range(len(data_loader.dataset)))\n",
    "            random.shuffle(shuffled_indices)\n",
    "            subset_indices = shuffled_indices[:data_loader.batch_size]\n",
    "            subset = Subset(data_loader.dataset, subset_indices)\n",
    "\n",
    "            # Create a batch sampler to iterate over the subset in batches\n",
    "            batch_sampler = BatchSampler(SequentialSampler(subset), batch_size=data_loader.batch_size, drop_last=False)\n",
    "\n",
    "            for batch_indices in batch_sampler:\n",
    "                batch = [subset[i] for i in batch_indices]\n",
    "\n",
    "                batch_inputs = {\n",
    "                    'input_ids': torch.stack([sample['input_ids'] for sample in batch]).to(device),\n",
    "                    'attention_mask': torch.stack([sample['attention_mask'] for sample in batch]).to(device),\n",
    "                }\n",
    "                batch_labels = torch.stack([sample['targets'] for sample in batch]).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = edited_model(**batch_inputs)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = loss_fn(logits, batch_labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Backpropagation to update the parameter of the chosen model\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        average_loss = total_loss / num_batches\n",
    "        losses.append(average_loss)\n",
    "\n",
    "    # Plot size\n",
    "    plt.figure(figsize=(15, 9))\n",
    "\n",
    "    # plotting a large zoomed out graph and outputing the lowest loss point\n",
    "    if lr_finder == False :\n",
    "      plt.plot(lr_values, losses)\n",
    "      plt.xscale('log')\n",
    "      plt.xlabel('Learning Rate')\n",
    "      plt.ylabel('Loss')\n",
    "      plt.title('Learning Rate Range Test')\n",
    "\n",
    "      best_lr_index = losses.index(min(losses))\n",
    "      lowest_lr = lr_values[best_lr_index]\n",
    "\n",
    "      plt.scatter(lowest_lr, min(losses), color='red', marker='o', label=f'Lowest Loss : {lowest_lr:.0e}')\n",
    "\n",
    "      plt.legend()  # Add a legend to label the red dot\n",
    "      plt.show()\n",
    "\n",
    "      print(f\"Learning rate value of the lowest loss value point is: {lowest_lr:.0e}\")\n",
    "      \n",
    "      return lowest_lr\n",
    "      \n",
    "    # plotting a zoomed in graph and  outputing the elbow point as the best option  \n",
    "    else :\n",
    "      # Calculate differences between consecutive losses\n",
    "      loss_diffs = [losses[i] - losses[i - 1] for i in range(1, len(losses))]\n",
    "      # Calculate second differences between consecutive loss differences\n",
    "      loss_diff_diffs = np.diff(loss_diffs)\n",
    "\n",
    "      # Find the index of the point of inflection (elbow)\n",
    "      inflection_index = np.argmax(loss_diff_diffs) + 1\n",
    "\n",
    "      # Choose the learning rate associated with the point of inflection\n",
    "      best_lr = lr_values[inflection_index]\n",
    "\n",
    "      # Plot the results\n",
    "      plt.plot(lr_values, losses)\n",
    "      plt.scatter(best_lr, losses[inflection_index], color='green', label=f'Best LR: {best_lr:.0e}')\n",
    "      plt.xscale('log')\n",
    "      plt.xlabel('Learning Rate')\n",
    "      plt.ylabel('Loss')\n",
    "      plt.title('Learning Rate Range Test')\n",
    "      plt.legend()\n",
    "      plt.show()\n",
    "\n",
    "      print(f\"Learning rate value at the point of inflection (elbow) is: {best_lr:.0e}\")\n",
    "\n",
    "      return best_lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tunning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_bert(model, train_loader, loss_fn, optimizer, num_epochs=5, scheduler=None):\n",
    "\n",
    "    # devine to set everything either through gpu calculation or cpu,but atleast synchronized\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    loss_fn.to(device)\n",
    "    \n",
    "    output_layer_index = -1  \n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Freezing all layers expect the last one (I have tested unfreezing as you go method,or unfreezing all layers and  2 other method the best one is freezing all of them expect thel ast one)\n",
    "    for idx, param in enumerate(model.parameters()): \n",
    "        if idx < output_layer_index:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    # Going through the  epochs with a beautiful tqdm bar progression\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        all_predicted_tags = []\n",
    "        all_targets = []\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        \n",
    "        # Going through  the  dataset using batchs (size = 16 in  this case),and  updating the  parameters of the  model  after each batch by a forward  pass,loss calculation and a backpropagation process and I added a  schedule option if needed (Cosine  , ReduceLROnplateau or decayingLR  ect . .. )\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs.logits, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            predicted_probs = torch.sigmoid(outputs.logits) # Sigmoid because  it is a  multi label text classification,and softmax if it is a  mutli class task\n",
    "            predicted_tags = (predicted_probs >= 0.5).float()\n",
    "            \n",
    "            total_correct += torch.sum(torch.eq(predicted_tags, targets)).item()\n",
    "            total_samples += len(targets)\n",
    "            \n",
    "            all_predicted_tags.extend(predicted_tags.cpu())\n",
    "            all_targets.extend(targets.cpu())\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': loss.item(),\n",
    "            })\n",
    "            \n",
    "        # Registering some evaluiation metrics \n",
    "        all_predicted_tags = torch.stack(all_predicted_tags)\n",
    "        all_targets = torch.stack(all_targets)\n",
    "        \n",
    "        predicted_np = all_predicted_tags.numpy()\n",
    "        targets_np = all_targets.numpy()\n",
    "        \n",
    "        f1_micro = f1_score(targets_np, predicted_np, average='micro')\n",
    "        f1_macro = f1_score(targets_np, predicted_np, average='macro')\n",
    "        \n",
    "        jaccard_micro = jaccard_score(targets_np, predicted_np, average='micro')\n",
    "        jaccard_macro = jaccard_score(targets_np, predicted_np, average='macro')\n",
    "        \n",
    "        avg_accuracy = total_correct / total_samples\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {(train_loss/len(train_loader)):.5f} - Accuracy: {avg_accuracy:.3f} - F1 Micro: {f1_micro:.4f} - F1 Macro: {f1_macro:.4f} - Jaccard Micro: {jaccard_micro:.4f} - Jaccard Macro: {jaccard_macro:.4f}\" )\n",
    "        \n",
    "        #Schedule option \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggesting tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(model, input_question, threshold=0.5):\n",
    "    input_question.lower()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    input_ids = tokenizer.encode(input_question, max_length=tokenizer.model_max_length, truncation=True)\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    \n",
    "    predicted_probabilities = torch.nn.functional.sigmoid(outputs.logits)   # Sigmoid because  it is a  multilabel text classification,and softmax if it is a  mutli class task\n",
    "    tag_probabilities = predicted_probabilities.squeeze().tolist()\n",
    "    \n",
    "    total_probability = round(sum(tag_probabilities) , 6)\n",
    "\n",
    "    tag_labels = y.columns  \n",
    "\n",
    "    decimal_places = 3\n",
    "    \n",
    "    tag_predictions = [{\"tag\": tag_labels[i], \"probability\": prob} for i, prob in enumerate(tag_probabilities) if prob > threshold]\n",
    "    \n",
    "    sorted_tag_predictions = sorted(tag_predictions, key=lambda x: x[\"probability\"], reverse=True)\n",
    "\n",
    "    print(f\"Here is the sum of the total probability of all 60 tags :  {total_probability} \" )\n",
    "\n",
    "    # Print the sorted tag predictions above the threshold\n",
    "    for tag_prediction in sorted_tag_predictions:\n",
    "        rounded_probability = round(tag_prediction['probability'], decimal_places)\n",
    "        print(f\"Tag: {tag_prediction['tag']}, Probability: {rounded_probability}\")\n",
    "    \n",
    "    #returning the suggested tags in an array format\n",
    "    return [item['tag'] for item in sorted_tag_predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structure of my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, questions, targets, tokenizer, max_length):\n",
    "        self.questions = questions\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        question = str(self.questions[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            question,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.float),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLITING DATA INTO TRAIN/TEST INPUT/OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data into train and test sets\n",
    "train_df, test_df = train_test_split(df_MLB, test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract questions and corresponding tags\n",
    "train_questions = train_df['Questions'].tolist()\n",
    "train_tags = train_df.drop(columns=['Questions']).values.tolist()\n",
    "\n",
    "test_questions = test_df['Questions'].tolist()\n",
    "test_tags = test_df.drop(columns=['Questions']).values.tolist()\n",
    "\n",
    "y = df_MLB.drop('Questions' , axis=1)\n",
    "all_tags = y.columns\n",
    "labels_number = all_tags.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEARCHING FOR THE BEST MAX_LENGTH VALUE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of words for each sentence\n",
    "sentence_lengths = df_MLB['Questions'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Create a histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sentence_lengths, bins=50, color='blue', alpha=0.7)\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average number of words\n",
    "average_num_words = df_MLB['Questions'].apply(lambda x: len(x.split())).mean()\n",
    "\n",
    "# Calculate the maximum number of words\n",
    "max_num_words = df_MLB['Questions'].apply(lambda x: len(x.split())).max()\n",
    "\n",
    "# Calculate the minimum number of words\n",
    "min_num_words = df_MLB['Questions'].apply(lambda x: len(x.split())).min()\n",
    "\n",
    "# Safety margin (1/3 for this case)\n",
    "safety_margin = 0.33*average_num_words\n",
    "\n",
    "# Calculate max_length based on average_num_words + safety_margin\n",
    "max_length = int(average_num_words + safety_margin)\n",
    "\n",
    "print(f\"Average number of words per sentence: {average_num_words:.2f}\")\n",
    "print(f\"Maximum number of words per sentence: {max_num_words}\")\n",
    "print(f\"Minimum number of words per sentence: {min_num_words}\")\n",
    "print(f\"Setting max_length to: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading / preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set maximum sequence length\n",
    "max_seq_length = 256\n",
    "batch_size_number  =  16\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create DataLoader for training and testing data\n",
    "train_dataset = CustomDataset(train_questions, train_tags, tokenizer, max_seq_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size_number, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(test_questions, test_tags, tokenizer, max_seq_length)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size_number, shuffle=False)\n",
    "\n",
    "# Load the pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=labels_number)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATING THE BASE BERT MODEL DIRECTLY WITHOUT FINE TUNNING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(model, train_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINDING  THE  BEST LEARNING RATE VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the first range \n",
    "lr_values  = [1e-7 , 1e-6  , 1e-5 , 1e-4 , 1e-3 ,  1e-2  , 1e-1]\n",
    "\n",
    "lowest_loss_lr = lr_best_finder(model , train_loader , loss_fn , lr_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the real precise value\n",
    "lr_values = [1e-5 , 2e-5, 3e-5 , 4e-5 , 5e-5 , 6e-5 , 7e-5  , 8e-5 , 9e-5 , 1e-4 , 2e-4 , 3e-4]\n",
    "\n",
    "best_lr_value = lr_best_finder(model , train_loader , loss_fn , lr_values , lr_finder = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINE TUNNING PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning the BERT model\n",
    "num_epochs = 6\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=best_lr_value)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler_cosine = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "fine_tune_bert(model, train_loader, loss_fn, optimizer, num_epochs=num_epochs, scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVING THE FINE TUNNED MODEL (default : Google Collab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "model_path = \"/content/drive/My Drive/bert_model_2e-5.pth\"  # This path  is for  google collab user,change it at your will\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "# This below is the code to load the model localy\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "# # Get the current working directory (directory of the Jupyter Notebook)\n",
    "# notebook_directory = %pwd\n",
    "\n",
    "# model_filename = \"bert_model_2e-5.pth\"\n",
    "# model_path = os.path.join(notebook_directory, model_filename)\n",
    "\n",
    "# # Assuming you have a 'model' variable containing your model\n",
    "# torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADING THE FINE TUNNED MODEL (default : Google Collab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=labels_number\n",
    ")\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "model_path = \"/content/drive/My Drive/bert_model_2e-5.pth\"  \n",
    "\n",
    "loaded_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "# This below is the code to load the model localy\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "# # Create the model instance\n",
    "# loaded_model = BertForSequenceClassification.from_pretrained(\n",
    "#     model_name,\n",
    "#     num_labels=labels_number\n",
    "# )\n",
    "\n",
    "# # Get the current working directory (directory of the Jupyter Notebook)\n",
    "# notebook_directory = %pwd\n",
    "\n",
    "# # Load the saved model state dictionary\n",
    "# model_filename = \"bert_model_2e-5.pth\"\n",
    "# model_path = os.path.join(notebook_directory, model_filename)\n",
    "\n",
    "# loaded_model.load_state_dict(torch.load(model_path)) # NOTE : The model has been fine tunned using T4 GPU,loading it with a CPU will not work !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATING THE FINE TUNNED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(loaded_model, test_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTING TAGS WITH AN INPUT QUESTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_row =  4234\n",
    "\n",
    "questions1 = df_MLB['Questions'][index_row]\n",
    "input_test = 'How can I fine tune a BERT model ?'\n",
    "input_test_1 = 'How can I input an image into my website ?'\n",
    "input_test_2 = 'How to add interactivity with to my web site in a declarative way on the front end?'\n",
    "input_test_3 = 'What is the difference between high level and low level language programming ?'\n",
    "input_test_4 = 'How to creat a boss script using c# in a video game ?'\n",
    "\n",
    "\n",
    "\n",
    "input_question = input_test_3\n",
    "\n",
    "preprocessed_question = preprocessed_text(input_question)\n",
    "\n",
    "suggested_tags_BERT = predict_tags(loaded_model, input_question, threshold=0.1)\n",
    "\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Real tags :\", df_OG_TT['Tags'][index_row])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
